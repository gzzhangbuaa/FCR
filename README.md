# ***___FCR___***
A library implemented in the user level to provide supporting for failures distinguishing  
(face to large-scale MPI applications)   
## ***___APPENDIX___***    
### **___A. Abstract___**     
This artifact contains the code to build the FCR library and the information needed to launch some experiments in the paper “A Lightweight and Flexible Tool for Distinguishing between Hardware Malfunctions and Program Bugs in Debugging Large-scale Programs”. We explain how to compile and run the modified benchmarks used in Section IV.
### **___B. Description___**
#### **1)	Check-list (artifact meta information):**    
**•	Program:** C and MPI code, C library    
**•	Compilation:** mpiicc (icc-14.0.2) with the -O2 flag    
**•	Binary:** HPL, HPCG, HPCC executables   
**•	Data Set:** default input of the three benchmarks but need small modification according to the demand for running at scale     
**•	OS environment:** Red Hat Enterprise Linux Server release 6.5 (Santiago) kernel version-2.6.32, intel-MPI-5.0.2     
**•	Hardware:** Tianhe-2 supercomputer, each node has  two Intel Xeon E5-2692 v2 CPU with 24 cores and 64GB memory. We use up to 256 nodes to complete the overhead evaluation in Section IV.    
**•	Output:** 	execution time, status log    
**•	Experiment workflow:** build FCR library, modify benchmarks code with APIs provided by FCR, submit a job script to the PBS batch system, wait for the job to be scheduled, and then collect the results.      
**•	Experiment customization:** modify the benchmarks by adding FCR library calls and simulating hardware malfunctions or software bugs, modify configure file for running at different scales    
**•	Publicly available?:** Yes.   

#### **2)	How delivered:**    
      HPL, HPCG and HPCC are open- source benchmarks, you can get them in the following URLs separately: 
      http://www.netlib.org/benchmark/hpl/    
      http://www.hpcg-benchmark.org/        
      http://icl.cs.utk.edu/hpcc/   
#### **3)  Hardware dependencies:**     
We used Tianhe-2 supercomputer for performance evaluation and functional verification.      
#### **4)  Software dependencies:**
HPL depends on BLAS or Intel MKL. We used the existing HPL on Tianhe-2 for our experiments in Section IV, which uses Intel MKL as its math library.
#### **5)  Datasets:**
The performance evaluation requires running the target application with and without FCR under various running scales. It’s necessary to adjust the corresponding parameters in the input files including problem size and running scales, as well as the content in submit script.
### **___C. Installation___**     
Build a static library of FCR as libpdi.a and an executable named child.    
      $ cd pdi/src    
      $ ./make.sh 
### **___D. Experiment workflow___**
Taking the experiments of HPL as an example, the first step is to add FCR to HPL. Modify linpack/testing/ptest/ HPL_pddriver.c, linpack/src/comm/HPL_Send.c and HPL_Recv.c as Figure 4 described in this paper. Compiling the modified HPL code to generate an executable of HPL (with-FCR version).      
      $ cp pdi/src/libpdi.a     linpack/lib/     
      $ cp pdi/include/pdi.h    linpack/include/    
      $ cd linpack/     
      $ make arch=intel64     
      $ cp pdi/src/child    linpack/bin/intel64    
Create job.sh for submitting to Tianhe-2.     
      #!/bin/sh     
      yhrun –N 256 –n 5120 –exclusive –p bigdata ./xhpl      
### **___E. Evaluation and expected result___**
#### **1) Effectiveness:**      
For evaluating the effectiveness of FCR, we choose to emulate hardware malfunctions by stopping all the processes belong to the target execution on a certain computing node, and simulate software bugs by stopping certain MPI process to cause program hanging. The above failure injections have been implemented in FCR, you can inject the failure anywhere in the program with function call PDI_FailureTest(int nodeID, int type) before compilation. The first parameter nodeID represents the node you want the failure occur, and the second parameter represents the failure type such as hardware (PDI_TYPE_HFAIL) and software (PDI_TYPE_SFAIL).      
Determine the problem size and the running scale you want to evaluate, update the input file of HPL.
      line 6    80000    Ns    
      line 11   256       Ps      
      line 12    20        Qs      
Submitting the job you want to run.
      $ cd linpack/bin/intel64      
      $ yhbatch –N 256 –n 5120 –p bigdata job.sh     
#### **2) Performance overhead:**
We use the same running scale, problem size and process number per node to evaluate the original HPL and the modified HPL with FCR. Then we vary running scale from 4 to 256, problem size from 40% to 80% of memory occupancy and MPI processes per node from 4 to 20 respectively. The overhead caused by FCR is shown in Figure 7~Figure 9. The program execution is recorded in log file “slurm-xxx.out”. In addtion, there are several status reports generated by FCR during runtime. If the execution ends normally, there is only one log file (i.e. allhost.log), however if failure occurs during program run, two other log files (i.e. suspect.log and nstatus.log) are generated by FCR. Users can identify the hardware malfunctions and locate the source of the failure by analyzing these status reports.     
      *___allhost.log___*	A list of nodes participated in the execution. Each element is a key-value pair (e.g. <1, cn8452>).     
      *___suspect.log___*	Each entry represents a record of suspicious event.     
      *___nstatus.log___*	The hardware status of all nodes participated in the execution at the failure point.     
### **___F. Experiment customization___**     
Before running the experiment, one should determine the execution parameters, and adjust the configuration file correspondingly. For example, to perform HPL on 256 nodes, each node with 16 processes and each process takes 80% memory occupancy, one should adjust the HPL.dat file as follows.      
      line 6    80000    Ns     
      line 11   256       Ps      
      line 12    16        Qs      
Then modify job.sh as follows:      
      #!/bin/sh       
      yhrun –N 256 –n 4096 –exclusive –p bigdata ./xhpl        
Finally, submit this job to the Tianhe-2 supercomputer:        
      $ cd linpack/bin/intel64        
      $ yhbatch –N 256 –n 4096 –p bigdata job.sh        
      
### **___G. Notes___**       
We take the experiments of HPL as an example to illustrate how to compile and run the program with FCR. The experiments with the rest of the benchmarks follows the similar steps. It is necessary to adjust the makefile of benchmarks in order to compile with FCR library and generate the executables.








      
      






      
      

      







